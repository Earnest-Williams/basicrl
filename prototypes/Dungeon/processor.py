# Dungeon/processor.py

import copy  # To avoid modifying original data
from typing import Dict, List, Tuple

import numpy as np

# --- Constants (Ensure these are consistent with shaper.py if needed) ---
GRID_RESOLUTION = 1.0  # meters per grid cell

# --- (Heuristic Thresholds are NO LONGER NEEDED here) ---


def process_backbone_graph(backbone_data: Dict) -> Tuple[List[Dict], Dict[int, Dict]]:
    """
    Interstitial step to process raw backbone graph data from core.py.
    Calculates segment geometry (length, incline) using simple linear depth_m.
    Passes along feature flags generated by core.py.
    Adds calculated geometry to the child node's data dict.

    Args:
        backbone_data: The dictionary loaded from CaveGenerator.to_json(),
                       containing 'nodes' list (potentially with 'feature' keys).

    Returns:
        Tuple containing:
        - augmented_nodes (List[Dict]): A deep copy of the nodes list, with added
            segment geometry keys where applicable (flags are passed through).
        - augmented_node_map (Dict[int, Dict]): A map from node ID to the
            corresponding augmented node dictionary.
    """
    if "nodes" not in backbone_data:
        print("Warning: 'nodes' key not found in backbone_data.")
        return [], {}

    # Create deep copies to work with
    nodes_list = copy.deepcopy(backbone_data["nodes"])
    node_map = {n["id"]: n for n in nodes_list}

    print(f"Processor: Calculating geometry for {len(nodes_list)} nodes...")

    # Iterate through the copied nodes to calculate segment geometry
    for node_data in nodes_list:
        parent_id = node_data.get("parent_id")

        # Skip root or nodes with missing/invalid parents
        if parent_id is None or parent_id not in node_map:
            # Add default segment values for root if needed downstream?
            node_data["segment_length_xy"] = 0.0
            node_data["segment_incline_rate"] = 0.0
            node_data["segment_delta_depth_m"] = 0.0
            continue

        parent_data = node_map[parent_id]

        # --- Calculate Segment Geometry ---
        delta_x = node_data["x"] - parent_data["x"]
        delta_y = node_data["y"] - parent_data["y"]
        segment_length_xy = np.hypot(delta_x, delta_y)

        # Use the simple 'depth_m' calculated by core.py
        delta_depth_m = node_data.get(
            "depth_m", 0.0) - parent_data.get("depth_m", 0.0)

        incline_rate = 0.0
        if segment_length_xy < 0.01:  # Avoid division by zero
            # Handle vertical segments if needed (e.g., assign large incline)
            # However, incline rate isn't very meaningful here if core.py flagged it.
            # Let's keep it simple.
            pass
        else:
            incline_rate = delta_depth_m / segment_length_xy

        # Add calculated geometry to the *child* node's dictionary
        node_data["segment_length_xy"] = round(segment_length_xy, 2)
        node_data["segment_incline_rate"] = round(incline_rate, 2)
        node_data["segment_delta_depth_m"] = round(delta_depth_m, 2)

        # --- Flagging is NO LONGER done here ---
        # Feature flags ('cliff_edge', 'shaft_opening') are assumed to be
        # already present in the parent_data if generated by core.py
        # The deep copy ensures they are carried over.

    # Log counts of flags received from core.py
    cliff_count = sum(1 for n in nodes_list if n.get(
        "feature") == "cliff_edge")
    shaft_count = sum(1 for n in nodes_list if n.get(
        "feature") == "shaft_opening")
    print(
        f"Processor: Found {cliff_count} cliff edges, {
            shaft_count} shaft openings (generated by core)."
    )
    print("Processor: Geometry calculation complete.")

    return nodes_list, node_map


# --- Example Integration Snippet (if run directly for testing) ---
if __name__ == "__main__":
    import json

    # Assumes core.py generated 'generated_cave_contextual.json' or similar
    INPUT_JSON_FILE = "generated_cave_contextual.json"
    OUTPUT_PROCESSED_FILE = "processed_cave_data.json"

    try:
        with open(INPUT_JSON_FILE, "r") as f:
            raw_backbone_data = json.load(f)
        print(
            f"Loaded {len(raw_backbone_data['nodes'])} raw nodes from {
                INPUT_JSON_FILE}"
        )

        print("\n--- Running Interstitial Processing (Geometry Calculation Only) ---")
        augmented_nodes, augmented_node_map = process_backbone_graph(
            raw_backbone_data)

        # --- Optional: Save the augmented data ---
        output_data = {
            "nodes": augmented_nodes,
            "generation_settings": raw_backbone_data.get("generation_settings", {}),
        }
        with open(OUTPUT_PROCESSED_FILE, "w") as f:
            json.dump(output_data, f, indent=2)
        print(f"\nAugmented data saved to {OUTPUT_PROCESSED_FILE}")

        # Example: Check augmentation on a node known to have children
        example_parent_id = 3  # Example ID
        if example_parent_id in augmented_node_map:
            parent_node_example = augmented_node_map[example_parent_id]
            print(f"\nAugmented data for node {example_parent_id}:")
            print(parent_node_example)
            if parent_node_example.get("children_ids"):
                child_id = parent_node_example["children_ids"][0]
                if child_id in augmented_node_map:
                    print(f"\nAugmented data for its child node {child_id}:")
                    print(augmented_node_map[child_id])

    except FileNotFoundError:
        print(f"Error: Input file '{INPUT_JSON_FILE}' not found.")
        print("Please generate it by running the updated core.py first.")
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback

        traceback.print_exc()
