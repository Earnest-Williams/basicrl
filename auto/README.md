# simple_rl/auto - GOAP AI Simulation Environment

## Purpose

This directory contains the simulation environment and core AI implementation for **combat-oriented agents** (adventurers, monsters) designed to operate within hostile environments like the procedurally generated dungeons (`simple_rl/Dungeon`). It utilizes the Goal-Oriented Action Planning (GOAP) paradigm to drive agent behavior.

This component serves as the primary development and testing environment for the GOAP AI (`AgentAI`, `GOAPPlanner`) before its intended integration into the main game engine to control NPCs and monsters in dungeon/cave settings.

## Core Functionality

* **Simulation Environment (`simulation.py`):**
    * Defines a grid-based `World` using `numpy` (implicitly via Polars) and standard Python collections.
    * Manages `Entity` states (Agent, Enemy, Slime, Item) efficiently using a **Polars DataFrame** (`world.entity_df`) for scalable state storage and querying.
    * Includes basic physics/interactions: movement, health/hunger tracking, simple combat, item interaction (pickup, consume, equip), enemy spawning, and food respawning.
    * Provides an A\* pathfinding implementation (`world.find_path`) using `heapq`.
    * Uses Numba (`@njit`) to accelerate distance calculations.
* **GOAP AI (`simulation.py`):**
    * **`Action`:** Represents atomic actions agents can perform (MoveTo, Attack, Flee, Pickup, Consume, Equip, Wait, Explore) with defined costs, preconditions, and effects.
    * **`GOAPPlanner`:** Implements an A\* search over the available actions to find a sequence (plan) that achieves a desired goal state from the current world state. Action costs are weighted based on learned effectiveness.
    * **`AgentAI`:** Orchestrates the agent's turn. It determines the current goal (`_select_goal`), requests a plan from the `GOAPPlanner`, executes the plan step-by-step, validates plan validity, handles replanning, and triggers learning.
    * **Learning:** The AI learns by adjusting the weights associated with actions (`planner.action_weights`). After a simulation run, weights are updated based on the agent's survival outcome (turns survived, final health), making successful action sequences cheaper (more likely to be chosen) in the future. Unlike the community AI (`simple_rl/AI`), this system starts with functional knowledge (e.g., how to eat) and learns the *value* and consequences of actions.
* **Execution & Testing (`main.py`, `run.sh`):**
    * `main.py` provides a headless runner capable of executing multiple simulation runs in parallel using `multiprocessing.Pool`.
    * Supports different learning modes (`independent` vs. `shared` weights across runs).
    * Collects and summarizes simulation results (survival rates, turns, final weights).
    * Includes optional `cProfile` integration for performance analysis.
    * `run.sh` provides a convenient wrapper to execute `main.py`.
* **GUI (`gui/` subfolder):**
    * A PySide6-based graphical interface (`main_window.py`, `gui_widgets.py`) for visualizing the simulation state and agent plans in real-time.
    * Uses a separate thread (`worker.py`) to run the simulation, preventing GUI freezes.
    * Primarily intended as a **development and tuning tool**, not for final gameplay integration.

## Dependencies

* **Python:** 3.x
* **Core Libraries:**
    * `polars`: For efficient entity state management.
    * `numpy`: Used by Polars and potentially in other calculations.
    * `numba`: For JIT-accelerating the distance function.
* **GUI Specific (Optional):**
    * `PySide6`: For the graphical interface.
* **Development/Testing:**
    * `random`: Used for world events like spawning (Consider replacing with `GameRNG` for full determinism if needed within this simulation).
    * Standard libraries: `collections`, `heapq`, `time`, `uuid`, `argparse`, `multiprocessing`, `cProfile`, `io`, `pstats`, `sys`, `os`.

## Usage

* **Headless Mode (for batch runs/testing):**
    ```bash
    ./run.sh --mode headless -n <num_runs> -w <num_workers> --learn <independent|shared>
    # Example: ./run.sh --mode headless -n 100 -w 4 --learn shared
    ```
* **GUI Mode (for visualization/debugging):**
    ```bash
    ./run.sh --mode gui
    ```

## Integration

This component is designed to provide the AI logic for hostile/adventuring NPCs interacting within the dungeon environments generated by `simple_rl/Dungeon`. The exact integration points with the main game loop and other systems (like perception from `simple_rl/pathfinding`) are still under development.
